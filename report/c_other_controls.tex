\section{Turbine control considering model's uncertainty}\label{sec:c_other_controls}
The control methods developed so far relay on the knowledge of the system by means of their parameters (structural and environmental) and models. In fact in the calculation of the gains $K_{opt}$ and $K_{opt,GE}$, considered in the torque reference generation, a single deterministic constant in time value of all the different terms in \autoref{eq:T_G2} \textcolor{red}{check this reference} have been used. In order to take into account possible variabilities of the parameters, for example due to the wrong estimation of some quantities or ageing, a different approach has to been used. This is particularly important nowadays because the turbines that are going to be set in these years will have to operate for at least 20 years, while the global worming will change the average environmental conditions. For example the increase of temperature would lead to a air density change of 0.5\% in the site presented in \cite{en12112038}.

One possibility is to use model-free methods, such as the extremum seeking, while another is to suppose that the system may evolve following a model within a finite set, without explicitly defining which, such as the \acrfull{IMM}. Another advantage of this last method is to be able to provide an estimation of the value of $K_{opt}$ to be used in the feedback line of the controller. 

\subsection{Comparison of the electrical power output in the case of different gains in the power controller}\label{subsec:c_different_KoptGE}
The first study done to identify the effect of a changing $K_{opt,GE}$ is to compare the electrical power output of the generator for its  different values. In particular, a simulation with 5 values in the neighborhood of the optimal has been identified (second column of \autoref{tab:error_KoptGE}) and then a ramp WS profile between 4 and 12 $\mesunt{\meter\per\second}$ lasting 1000 $\mesunt{\second}$ has been applied.
\begin{table}[htb]
  \centering
  \begin{tabular}{cc|cc}
  \toprule
  Sim. & K & RMS error & Norm. RMS error\\ 
   &  $\mesunt{\newton \meter \square \second}$ & $\mesunt{\mega\watt}$ & \% $\left[-\right]$ \\ \midrule
  1 & 0.85$K_{opt,GE}$  & 1.20 & 16.01\\
  2 & 0.90$K_{opt,GE}$  & 0.75 & 11.15\\
  3 & 1.00$K_{opt,GE}$  & 0.08 & 6.90\\
  4 & 1.10$K_{opt,GE}$  & 0.12 & 7.00\\
  5 & 1.15$K_{opt,GE}$  & 0.14 & 7.13\\ \bottomrule
  \end{tabular}
  \caption{Error with the use of different $K_{opt,GE}$ values}
  \label{tab:error_KoptGE}
\end{table}

The resulting static power curve has been reported in \autoref{fig:Kopt_GE_PGE}.
\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{images/vectorial/2023_10_18_14_54_25fig_electrical_power_param_zoom.eps}
  \caption{Electrical power output of the generator for different values of $K_{opt,GE}$. The values of $K_{opt,GE}$ used in each simulation are the ones reported in \autoref{tab:error_KoptGE}.}
  \label{fig:Kopt_GE_PGE}
\end{figure}

From the graphs reported in \autoref{fig:Kopt_GE_PGE} it could be seen that the gain used in the first two simulations provides a power curve that is quite close to the third one (i.e. the optimal) in the below rated region. On the other hand, once the rated WS is reached, then the power command is below the saturation and so the rated power is not reached. The committed errors in this zone (16.01\% and 11.15\% respectively) are highly influenced by the above rated mismatch. \\
For what concerns the last two simulations, the error committed in the below rated region is higher (as could be seen in the figure) but the saturation of the power is reached, and so the overall error is lower than the first two cases. \\
From this simulation can be seen that the optimal gain is the one computed previously, and each deviation produces a degradation of the power curve. On the other hand, an overestimation of the gain is less worse than an underestimation, because in the latter case the saturation of the generator's torque command and so the output power is not achieved.

\subsection{Interactive Multiple Model}\label{subsec:IMM}
\textcolor{red}{What does a filter is? }\\
\textcolor{red}{What does a EKF is and which are its properties}\\
\textcolor{red}{Describe the conditions on the noise of the filter.}\\
The basic idea behind the IMM is first to run a parallel bank of $N_j$ filters estimating the same quantity, each of them using a different model of the system. Later on, the overall estimation of interest is given by the sum of all filters' output weighted by their likelihood conditioned on the measurement. In principle these filters may be of any type, such as Unscented Kalman Filter (UKF), but for its simplicity the EKF has been adopted here. \\
\subsubsection{Motivation of the IMM}
\subsubsection{Steps of the implementation of the IMM}
Here the procedure for implementing the IMM is reported, following the method proposed in \cite{kalman_based_IMM}. In particular the estimation of the quantity is given in 4 subsequently steps named filtering, mode probability updating, state combination and filter interaction. A graphical visualization of the method is given in \autoref{fig:IKK_schema}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\columnwidth]{images/IMM_schema.jpg}
  \caption{Graphical visualization of a IMM, source \cite{kalman_based_IMM}}
  \label{fig:IKK_schema}
\end{figure}

\textit{Adopted notation}\\
\textcolor{red}{Coherence on the notation of subscript k}
\begin{gather}
  \text{State of the \textit{j-th} filter: } x^{(j)} = \omega_{R} \\
  \text{State of the IMM: } \hat{x} = \omega_R\\
  \text{Input: } u = T_{R}\\
  \text{Measured quantity: } y = \omega_{R} \\
  \text{Model of the plant: }  x_{k+1} = f(x_k, u_k, \nu_k) = x_k + \frac{(T_{R,k} - K_{opt,GE}x_k^2 - B_{eq}x_k)}{I_{eq}} + \nu_{k}\\
  \text{Noise on the dynamic: } \nu_k \sim \mathcal{N}(0, Q) \\
  \text{Model of the measurement instrument: } y = h(x, u, \varepsilon) = \omega_{R} + \varepsilon \\
  \text{Noise on the measurement: } \varepsilon \sim \mathcal{N}(0, W)  \\
  \text{Covariance matrix of the process: } Q = \left[\left(\frac{\omega_{rated}}{50 \cdot3}\right)^2\right] =\left[0.0068^2\right]\\
  \text{Covariance matrix of the measurement: } W = \left[\left(\frac{\omega_{rated}}{10 \cdot3}\right)^2\right] = \left[0.0338^2\right]\\
  \text{Covariance matrix of the state estimation for the \textit{j-th} filter, and for the IMM: } P^{(j)} \, P\\
  \text{Linearized model of the plant w.r.t. the states: } F = \frac{\partial f}{\partial x}(x, u, 0) = \left[-\frac{2\,K_{opt}}{I_{eq}}x - \frac{B_{eq}}{I_{eq}}\right]\\
  \text{Linearized model of the measurement w.r.t. the states: } H = \frac{\partial h}{\partial x}(x, u) = \left[1\right]\\
  \text{Linearized model of the noise: } G = \frac{\partial f}{\partial \nu}(x, u, 0)\\
  \text{Mode probability: } \mu^{(j)}\\
  \text{Likelihood of a filter: } \Lambda^{(j)}
\end{gather}

\begin{enumerate}
\item \textit{Filtering}\\
The EKF is based on two step, named prediction and update which will be here presented. The apex (j) stresses the fact that EKF algorithm has to be repeated for each of the $N_j$ filters. 

Prediction step: the state dynamic and covariance are propagated based on the model only 
\begin{gather}
  \hat{x}_k^{(j)-} = f(\hat{x}_{k-1}^{(j)+}, u_k, \nu_k)\\
  F_k^{(j)} = \frac{\partial f}{\partial x}(\hat{x}_{k-1}^{(j)+}, u_k, 0)\\
  G_k^{(j)} = \frac{\partial f}{\partial x}(\hat{x}_{k-1}^{(j)+}, u_k, 0)\\
  P_k^{(j)-} = F_k^{(j)} P_{k-1}^{(j)+} \left(F_k^{(j)}\right)^T + G_k^{(j)} Q_k \left(G_k^{(j)}\right)^T
\end{gather}
Update step: the state dynamic and covariance are modified based on a measurement 
\begin{gather}
  \hat{y}_k = h(\hat{x}_k^{(j)-}, u_k, \varepsilon_k)\\
  H_k = \frac{\partial h}{\partial x}(\hat{x}_k^{(j)-}, u_k)\\
  L_k = P_k^- H_k^T (H_k P_k^- H_k^T + R_k)^{-1}\\
  \hat{x}_k^+ = \hat{x}_k^- + L_k (y_k - \hat{y}_k)\\
  P_k^+ = (I - L_k H_k) P_k^-
\end{gather}

\item \textit{Mode probability updating}\\
In this step the mode probabilities are updated from the filter likelihood (i.e. how likely the filter provides a good state estimate from the measurement). This is done assuming that the error residuals are Gaussianly distributed.  
\begin{gather}
  \text{Residual: } z_k^{(j)} = y_k - \hat{y}_k^{(j)}\\
  \text{Uncertainty on the residual: }S_k^{(j)} = H_k^{(j)} P_k^{(j)-} (H_k^{(j)})^T + R_k^{(j)}\\
  \text{Likelihood: } \Lambda_k^{(j)} = \frac{1}{\sqrt{2 \pi \lvert S_k^{(j)} \rvert}} \exp{\left(-\frac{1}{2}\left(z_k^{(j)}\right)^T \left(S_k^{(j)}\right)^{-1} z_k^{(j)}\right)}\\
  \mu_k^{(j)+} = \frac{\mu_k^{(j)-}\Lambda_k^{(j)}}{\sum_{j} \mu_k^{(j)-}\Lambda_k^{(j)}}
\end{gather}
In the application presented in \cite{kalman_based_IMM}, the IMM is used only for an estimation of the state. Here, differently, there is also the necessity of identifying the model for closing the control loop, meaning that at each step a value of $\hat{K}_{opt,GE}$ has to be provided and used for generating the reference torque. To do so, two methods have been identified. The first is to assign the value of the model with the highest probability, while the second is to compute the mean of the models' gain weighted by their probabilities. The second approach is here preferred, because it takes into account the information of all the models run in parallel.
\begin{gather}
\hat{K}_{opt,GE,k} = \sum_j \mu_k^{(j)+} K_{opt,GE}^{(j)}
\end{gather}

\item \textit{State combination}\\
The output state of the IMM is computed by combining the state of each filter by the corresponding covariance:
\begin{gather}
  \hat{x}_k^+ = \sum_j \mu_k^{(j)+} \hat{x}_k^{(j)+}\\
  P_k^+ = \sum_j \mu_k^{(j)+} \left(P_k^{(j)+} + \left(\hat{x}_k^+ - \hat{x}_k^{(j)+}\right)\left(\hat{x}_k^+ - \hat{x}_k^{(j)+}\right)^T\right)
\end{gather}

\item \textit{Filter interaction}\\
In this last step, the filter with higher probabilities modify the estimates of the one with lower probabilities. Each filter is considered as a mode, and the switching process of the modes is modelled by a time-invariant Markov chain. The mode transition probability $\pi_{ik} = \Pi(i, k)$ describes how likely the mode \textit{i} is change to mode \textit{j}. $\Pi \in \mathbb{R}^{N_j \times N_j}$ is the mode transition matrix, and it is a design parameter. In this application it is a symmetric matrix, with $\pi_{ii}=0.99$ and $\pi_{ik}=\pi_{ki}=\frac{1 - 0.99}{N_j} \,\forall \, i\neq k $ as proposed in \cite{kalman_based_IMM}.

\begin{gather}
  \mu_{k+1}^{(j)-} = \sum_i \pi_{ij}\mu_k^{(i)+}\\
  \mu_k^{(i|j)-} = \frac{\pi_{ij}\mu_k^{(i)+}}{\mu_{k+1}^{(j)-}} = \frac{\pi_{ij}\mu_k^{(i)+}}{\sum_i \pi_{ij}\mu_k^{(i)+}}\\
  \tilde{x}_k^{(j)+} = \sum_i \mu_k^{(i|j)-} \hat{x}_k^{(i)+}\\
  \tilde{P}_k^{(j)+} = \sum_i \mu_k^{(i|j)-} \left(P_k^{(i)+} + \left(\tilde{x}_k^{(j)+} - \hat{x}_k^{(i)+}\right)\left(\tilde{x}_k^{(j)+} - \hat{x}_k^{(i)+}\right)^T\right)
\end{gather}
\begin{equation}
  \Pi = 
  \begin{bmatrix}
    \pi_{11} & \dots & \pi_{1j} & \\
    \vdots & \ddots & \\
    \pi_{j1} &  & \ddots \\
     & & & \pi_{N_jN_j}
  \end{bmatrix}
  \in \mathbb{R}^{N_j \times N_j}
\end{equation}
Then the filtering step is repeated, with the mixed state $\tilde{x}_k^{(j)+}$ and covariance $\tilde{P}_k^{(j)+}$ in the prediction.
\end{enumerate}

\subsubsection{Definition of the boundaries for the variable parameters}
As said before, the IMM requires a-priori definitions of the parameters of the models run in each filter. It could be easily understood how the choice of these parameters is crucial for letting the algorithm work properly. In fact if the true model is not in the considered set, then the algorithm will converge to the most similar one, that may be \textit{close enough} or not according to the design choice.\\ 
In order to define these models, a Monte Carlo approach has been used, since the relationship between physical parameters and the $K_{opt,GE}$ involves also the use of a lookup table, which cannot be treated analytically as it is.\\
The principle of the Monte Carlo method is to generate the distribution of a quantity by means of evaluating its value multiple times, each one with a different value of the parameters, taking them from a distribution.\textcolor{red}{write better the introduction to the Monte Carlo method}\\
The parameters that have to be taken from distributions in this case are: R, $\rho$, $V_0$, $\omega$, and $c_P$.\\
The distribution of $V_0$ is defined as normal with mean the rated value and a reasonable standard deviation. \\
For what concerns $R$, in principle one can use the $V_0$ distribution for computing the deflection of the blade, using a suitable model for the deformation. A simplified method employed here is to initially suppose a normal distribution of the radius with mean in the undeformed blade length and a chosen standard deviation, and then removing all the sample greater than the undeformed one, observing that the length cannot be increased but only decreased. This will produced a distribution without one of the tails. \\
The $c_P$ is obtained by the use of the usual lookup table. Alongside the one of WS, a rotational speed, and pitch angle distributions are defined. The combination of the first two defines a distribution for the TSR which can be used alongside the third for obtaining the $c_P$ by interpolating the lookup table. It must be noted that here a formal error is committed, because the $c_P$ table obtained by means of a static analysis with some values of $\rho$ and $R$ is here used in other configurations. A more formal analysis should consider a more advanced model for the $c_P$ variation. 

The parameters and assumptions used for generating the distributions are reported in \autoref{tab:parameters_KoptGE}. It must be noted that the choice of these parameters are realistic but arbitrary, and so the numerical value of the final result can be compromised by them.

\begin{table}[htb]
  \centering
  \begin{tabular}{lcccc}
  \toprule
  Parameter & Mean & Std & MU & Motivation\\ \midrule
  Rotational speed $\omega$ & 0.988 & 0.0169 & $\mesunt{\radian\per\second}$ & More than 99\% distribution within 5\% rated value\\
  Air density $\rho$ &  1.225 & 0.0204 & $\mesunt{\kilo\gram\per\cubic\meter}$ & More than 99\% distribution within 5\% nominal value \\
  Rotor radius $R$  & 89.17 & 1.3333 & $\mesunt{\meter}$ & More than 99\% distribution within 4 $\mesunt{\meter}$ deformation \\
  Wind speed $V_0$  & 11.44 & 0.5719 & $\mesunt{\meter\per\second}$ & More than 99\% distribution within 15\% nominal value \\
  Pitch angle $\theta$ & 0.0 & 0.0058 & $\mesunt{\radian}$ & More than 99\% distribution within  1 $\si{\degree}$ \\
 \bottomrule
  \end{tabular}
  \caption{Error with the use of different $K_{opt,GE}$ values}
  \label{tab:parameters_KoptGE}
\end{table}

\begin{figure}[H]
  \begin{subfigure}{0.5\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{images/vectorial/2023_10_19_15_17_27R_distribution}
    \caption{Distribution of the rotor radius}
    \label{fig:R_distribution}
  \end{subfigure}
  \begin{subfigure}{0.5\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{images/vectorial/2023_10_21_10_25_47K_GE_distribution}
    \caption{Distribution of the $K_{opt,GE}$ obtained by the Monte Carlo simulation}
    \label{fig:K_GE_distribution}
  \end{subfigure}
\end{figure}

Given the distribution of the $K_{opt,GE}$ gain it is possible to use it in order to design the IMM, by means of selecting a given number of gains between two values of the distributions such as the 1 and 99 percentile, $7.19 \, \cdot 10^7 \mesunt{\newton \meter \square \second}$ and $1.50 \cdot 10^7 \, \mesunt{\newton \meter \square \second}$ respectively. In this specific case, 5 values have been chosen, and their are identified by the purple dotted line in \autoref{fig:K_GE_distribution}. To be specific these values are: $\left\[0.72, 0.91, 1.11, 1.30, 1.49 \right\] \cdot 10^7 \, \mesunt{\newton\meter\square\second}$. 


\subsubsection{Results of the IMM} 
\textcolor{red}{Not done yet}