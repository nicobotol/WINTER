\section{IMM implementation}\label{sec:IMM_implementation}

Here the procedure for implementing the IMM is reported, following the method proposed in \cite{kalman_based_IMM}. In particular the estimation of the quantity is obtained through four subsequent steps named filtering, mode probability updating, state combination and filter interaction. A graphical visualization of the method is given in \autoref{fig:IKK_schema}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\columnwidth]{images/IMM_schema.png}
  \caption{Graphical visualization of a IMM, source \cite{kalman_based_IMM}}
  \label{fig:IKK_schema}
\end{figure}
It is worth nothing that the method proposed here is not strictly related to the WT field, but it can be applied to any dynamic system.

\textit{Adopted notation}\\
\begin{gather}
  \text{State of the \textit{j-th} filter at time k: } x^{(j)}_{k}  \\
  \text{Estimated state of the IMM: } \hat{x}_{k}\\
  \text{Input: } u_{k} \\
  \text{Measure: } y_{k} \\
  \text{Model of the plant: }  x_{k+1} = f(x_k, u_k, \nu_k) \\
  \text{Noise on the dynamic: } \nu_k \sim \mathcal{N}(0, Q) \\ \label{eq:noise_Q}
  \text{Model of the measurement instrument: } y_k = h(x_k, u_k, \varepsilon_k) \\
  \text{Noise on the measurement: } \varepsilon_k \sim \mathcal{N}(0, W)  \\
  \text{Covariance matrix of the process: } Q\\
  \text{Covariance matrix of the measurement: } W \\
  \text{Covariance matrix of the state estimation for the \textit{j-th} filter } P^{(j)}\\
  \text{Linearized model of the plant with respect to the states: } F_k = \frac{\partial f(x_k, u_k, 0)}{\partial x}\\
  \text{Linearized model of the measurement with respect to the states: } H_k = \frac{\partial h(x_k, u_k)}{\partial x}\\
  \text{Linearized model of the model with respect to the state: } G_k = \frac{\partial f(x_k, u_k, 0)}{\partial \nu}\\
  \text{Mode probability: } \mu^{(j)}\\
  \text{Likelihood of a filter: } \Lambda^{(j)}
\end{gather}

\begin{enumerate}
\item \textit{Filtering}

The EKF is based on two steps, named prediction and update which will be here presented. The apex (j) stresses the fact that EKF algorithm has to be repeated for each of the $N_j$ filters. It must be remembered that the KF/EKF work under the assumptions that the noises are uncorrelated, zero mean and Gaussian distributed \cite{Kalman_Filter_and_Its_Application}. 

Prediction step: the state dynamic and covariance are propagated based on the model only 
\begin{gather}
  \hat{x}_k^{(j)-} = f(\hat{x}_{k-1}^{(j)+}, u_k, \nu_k)\\
  F_k^{(j)} = \frac{\partial f}{\partial x}(\hat{x}_{k-1}^{(j)+}, u_k, 0)\\
  G_k^{(j)} = \frac{\partial f}{\partial x}(\hat{x}_{k-1}^{(j)+}, u_k, 0)\\
  P_k^{(j)-} = F_k^{(j)} P_{k-1}^{(j)+} \left(F_k^{(j)}\right)^T + G_k^{(j)} Q_k \left(G_k^{(j)}\right)^T
\end{gather}
Update step: the state dynamic and covariance are modified based on a measurement 
\begin{gather}
  \hat{y}_k = h(\hat{x}_k^{(j)-}, u_k, \varepsilon_k)\\
  H_k = \frac{\partial h}{\partial x}(\hat{x}_k^{(j)-}, u_k)\\
  L_k = P_k^- H_k^T (H_k P_k^- H_k^T + R_k)^{-1}\\
  \hat{x}_k^+ = \hat{x}_k^- + L_k (y_k - \hat{y}_k)\\
  P_k^+ = (I - L_k H_k) P_k^-
\end{gather}

\item \textit{Mode probability updating}\\
In this step the mode probabilities are updated from the filter likelihood (i.e. how likely the filter provides a good state estimate from the measurement). This is done assuming that the error residuals are Gaussian distributed.  
\begin{gather}
  \text{Residual: } z_k^{(j)} = y_k - \hat{y}_k^{(j)}\\
  \text{Uncertainty on the residual: }S_k^{(j)} = H_k^{(j)} P_k^{(j)-} (H_k^{(j)})^T + R_k^{(j)}\\
  \text{Likelihood: } \Lambda_k^{(j)} = \frac{1}{\sqrt{2 \pi \lvert S_k^{(j)} \rvert}} \exp{\left(-\frac{1}{2}\left(z_k^{(j)}\right)^T \left(S_k^{(j)}\right)^{-1} z_k^{(j)}\right)}\\
  \mu_k^{(j)+} = \frac{\mu_k^{(j)-}\Lambda_k^{(j)}}{\sum_{i} \mu_k^{(i)-}\Lambda_k^{(i)}}
\end{gather}

\item \textit{State combination}\\
The output state of the IMM is computed by combining the state of each filter by the corresponding covariance:
\begin{gather}
  \hat{x}_k^+ = \sum_j \mu_k^{(j)+} \hat{x}_k^{(j)+}\\
  P_k^+ = \sum_j \mu_k^{(j)+} \left(P_k^{(j)+} + \left(\hat{x}_k^+ - \hat{x}_k^{(j)+}\right)\left(\hat{x}_k^+ - \hat{x}_k^{(j)+}\right)^T\right)
\end{gather}

\item \textit{Filter interaction}\\
In this last step, the filters with the highest probabilities modify the estimates of the ones with lower probabilities. Each filter is considered as a mode, and the switching process of the modes is modelled by a time-invariant Markov chain. The mode transition probability $\pi_{ik} = \Pi(i, k)$ describes how likely the mode \textit{i} is change to mode \textit{j}. $\Pi \in \mathbb{R}^{N_j \times N_j}$ is the mode transition matrix, and it is a design parameter. 
\begin{gather}
  \mu_{k+1}^{(j)-} = \sum_i \pi_{ij}\mu_k^{(i)+}\\
  \mu_k^{(i|j)-} = \frac{\pi_{ij}\mu_k^{(i)+}}{\mu_{k+1}^{(j)-}} = \frac{\pi_{ij}\mu_k^{(i)+}}{\sum_i \pi_{ij}\mu_k^{(i)+}}\\
  \tilde{x}_k^{(j)+} = \sum_i \mu_k^{(i|j)-} \hat{x}_k^{(i)+}\\
  \tilde{P}_k^{(j)+} = \sum_i \mu_k^{(i|j)-} \left(P_k^{(i)+} + \left(\tilde{x}_k^{(j)+} - \hat{x}_k^{(i)+}\right)\left(\tilde{x}_k^{(j)+} - \hat{x}_k^{(i)+}\right)^T\right)
\end{gather}
\begin{equation}
  \Pi = 
  \begin{bmatrix}
    \pi_{11} & \dots & \pi_{1j} & \\
    \vdots & \ddots & \\
    \pi_{j1} &  & \ddots \\
     & & & \pi_{N_jN_j}
  \end{bmatrix}
  \in \mathbb{R}^{N_j \times N_j}
\end{equation}
Then the filtering step is repeated, with the mixed state $\tilde{x}_k^{(j)+}$ and covariance $\tilde{P}_k^{(j)+}$ in the prediction.
\end{enumerate}
